{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant( [[1,2],[3,4]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "aa = [1,2,3,4,5]\n",
    "\n",
    "for i in aa[:-1]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "a2 = range(0, 10 * 10, 10)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, initial_weights, activation_fn, use_batch_norm):\n",
    "\n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.name = \"With Batch Norm\" if use_batch_norm else \"Without Batch Norm\"\n",
    "\n",
    "        # Batch normalization needs to do different calculations during training and inference,\n",
    "        # so we use this placeholder to tell the graph which behavior to use.\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        # This list is just for keeping track of data we want to plot later.\n",
    "        # It doesn't actually have anything to do with neural nets or batch normalization.\n",
    "        self.training_accuracies = []\n",
    "\n",
    "        # Create the network graph, but it will not actually have any real values until after you\n",
    "        # call train or test\n",
    "        self.build_network(initial_weights, activation_fn)\n",
    "    \n",
    "    def build_network(self, initial_weights, activation_fn):\n",
    "        \"\"\"\n",
    "        Build the graph. The graph still needs to be trained via the `train` method.\n",
    "        \n",
    "        :param initial_weights: list of NumPy arrays or Tensors\n",
    "            See __init__ for description. \n",
    "        :param activation_fn: Callable\n",
    "            See __init__ for description. \n",
    "        \"\"\"\n",
    "        self.input_layer = tf.placeholder(tf.float32, [None, initial_weights[0].shape[0]])\n",
    "        layer_in = self.input_layer\n",
    "        for weights in initial_weights[:-1]:\n",
    "            layer_in = self.fully_connected(layer_in, weights, activation_fn)    \n",
    "        self.output_layer = self.fully_connected(layer_in, initial_weights[-1])\n",
    "   \n",
    "    def fully_connected(self, layer_in, initial_weights, activation_fn=None):\n",
    "\n",
    "        # Since this class supports both options, only use batch normalization when\n",
    "        # requested. However, do not use it on the final layer, which we identify\n",
    "        # by its lack of an activation function.\n",
    "        if self.use_batch_norm and activation_fn:\n",
    "            # Batch normalization uses weights as usual, but does NOT add a bias term. This is because \n",
    "            # its calculations include gamma and beta variables that make the bias term unnecessary.\n",
    "            # (See later in the notebook for more details.)\n",
    "            weights = tf.Variable(initial_weights)\n",
    "            linear_output = tf.matmul(layer_in, weights)\n",
    "\n",
    "            # Apply batch normalization to the linear combination of the inputs and weights\n",
    "            batch_normalized_output = tf.layers.batch_normalization(linear_output, training=self.is_training)\n",
    "\n",
    "            # Now apply the activation function, *after* the normalization.\n",
    "            return activation_fn(batch_normalized_output)\n",
    "        else:\n",
    "            # When not using batch normalization, create a standard layer that multiplies\n",
    "            # the inputs and weights, adds a bias, and optionally passes the result \n",
    "            # through an activation function.  \n",
    "            weights = tf.Variable(initial_weights)\n",
    "            biases = tf.Variable(tf.zeros([initial_weights.shape[-1]]))\n",
    "            linear_output = tf.add(tf.matmul(layer_in, weights), biases)\n",
    "            return linear_output if not activation_fn else activation_fn(linear_output)\n",
    "\n",
    "    def train(self, session, learning_rate, training_batches, batches_per_sample, save_model_as=None):\n",
    "\n",
    "        # This placeholder will store the target labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.output_layer))\n",
    "        \n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if self.use_batch_norm:\n",
    "            # If we don't include the update ops as dependencies on the train step, the \n",
    "            # tf.layers.batch_normalization layers won't update their population statistics,\n",
    "            # which will cause the model to fail at inference time\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        else:\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "        # Train for the appropriate number of batches. (tqdm is only for a nice timing display)\n",
    "        for i in tqdm.tqdm(range(training_batches)):\n",
    "            # We use batches of 60 just because the original paper did. You can use any size batch you like.\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(60)\n",
    "            session.run(train_step, feed_dict={self.input_layer: batch_xs, \n",
    "                                               labels: batch_ys, \n",
    "                                               self.is_training: True})\n",
    "        \n",
    "            # Periodically test accuracy against the 5k validation images and store it for plotting later.\n",
    "            if i % batches_per_sample == 0:\n",
    "                test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                                 labels: mnist.validation.labels,\n",
    "                                                                 self.is_training: False})\n",
    "                self.training_accuracies.append(test_accuracy)\n",
    "\n",
    "        # After training, report accuracy against test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                         labels: mnist.validation.labels,\n",
    "                                                         self.is_training: False})\n",
    "        print('{}: After training, final accuracy on validation set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If you want to use this model later for inference instead of having to retrain it,\n",
    "        # just construct it with the same parameters and then pass this file to the 'test' function\n",
    "        if save_model_as:\n",
    "            tf.train.Saver().save(session, save_model_as)\n",
    "\n",
    "    def test(self, session, test_training_accuracy=False, include_individual_predictions=False, restore_from=None):\n",
    "\n",
    "        # This placeholder will store the true labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # If provided, restore from a previously saved model\n",
    "        if restore_from:\n",
    "            tf.train.Saver().restore(session, restore_from)\n",
    "\n",
    "        # Test against all of the MNIST test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.test.images,\n",
    "                                                         labels: mnist.test.labels,\n",
    "                                                         self.is_training: test_training_accuracy})\n",
    "        print('-'*75)\n",
    "        print('{}: Accuracy on full test set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If requested, perform tests predicting individual values rather than batches\n",
    "        if include_individual_predictions:\n",
    "            predictions = []\n",
    "            correct = 0\n",
    "\n",
    "            # Do 200 predictions, 1 at a time\n",
    "            for i in range(200):\n",
    "                # This is a normal prediction using an individual test case. However, notice\n",
    "                # we pass `test_training_accuracy` to `feed_dict` as the value for `self.is_training`.\n",
    "                # Remember that will tell it whether it should use the batch mean & variance or\n",
    "                # the population estimates that were calucated while training the model.\n",
    "                pred, corr = session.run([tf.arg_max(self.output_layer,1), accuracy],\n",
    "                                         feed_dict={self.input_layer: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    self.is_training: test_training_accuracy})\n",
    "                correct += corr\n",
    "\n",
    "                predictions.append(pred[0])\n",
    "\n",
    "            print(\"200 Predictions:\", predictions)\n",
    "            print(\"Accuracy on 200 samples:\", correct/200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, initial_weights, activation_fn, use_batch_norm):\n",
    "        \"\"\"\n",
    "        Initializes this object, creating a TensorFlow graph using the given parameters.\n",
    "        \n",
    "        :param initial_weights: list of NumPy arrays or Tensors\n",
    "            Initial values for the weights for every layer in the network. We pass these in\n",
    "            so we can create multiple networks with the same starting weights to eliminate\n",
    "            training differences caused by random initialization differences.\n",
    "            The number of items in the list defines the number of layers in the network,\n",
    "            and the shapes of the items in the list define the number of nodes in each layer.\n",
    "            e.g. Passing in 3 matrices of shape (784, 256), (256, 100), and (100, 10) would \n",
    "            create a network with 784 inputs going into a hidden layer with 256 nodes,\n",
    "            followed by a hidden layer with 100 nodes, followed by an output layer with 10 nodes.\n",
    "        :param activation_fn: Callable\n",
    "            The function used for the output of each hidden layer. The network will use the same\n",
    "            activation function on every hidden layer and no activate function on the output layer.\n",
    "            e.g. Pass tf.nn.relu to use ReLU activations on your hidden layers.\n",
    "        :param use_batch_norm: bool\n",
    "            Pass True to create a network that uses batch normalization; False otherwise\n",
    "            Note: this network will not use batch normalization on layers that do not have an\n",
    "            activation function.\n",
    "        \"\"\"\n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.name = \"With Batch Norm\" if use_batch_norm else \"Without Batch Norm\"\n",
    "\n",
    "        # Batch normalization needs to do different calculations during training and inference,\n",
    "        # so we use this placeholder to tell the graph which behavior to use.\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "        # This list is just for keeping track of data we want to plot later.\n",
    "        # It doesn't actually have anything to do with neural nets or batch normalization.\n",
    "        self.training_accuracies = []\n",
    "\n",
    "        # Create the network graph, but it will not actually have any real values until after you\n",
    "        # call train or test\n",
    "        self.build_network(initial_weights, activation_fn)\n",
    "    \n",
    "    def build_network(self, initial_weights, activation_fn):\n",
    "        \"\"\"\n",
    "        Build the graph. The graph still needs to be trained via the `train` method.\n",
    "        \n",
    "        :param initial_weights: list of NumPy arrays or Tensors\n",
    "            See __init__ for description. \n",
    "        :param activation_fn: Callable\n",
    "            See __init__ for description. \n",
    "        \"\"\"\n",
    "        self.input_layer = tf.placeholder(tf.float32, [None, initial_weights[0].shape[0]])\n",
    "        layer_in = self.input_layer\n",
    "        for weights in initial_weights[:-1]:\n",
    "            layer_in = self.fully_connected(layer_in, weights, activation_fn)    \n",
    "        self.output_layer = self.fully_connected(layer_in, initial_weights[-1])\n",
    "   \n",
    "    def fully_connected(self, layer_in, initial_weights, activation_fn=None):\n",
    "        \"\"\"\n",
    "        Creates a standard, fully connected layer. Its number of inputs and outputs will be\n",
    "        defined by the shape of `initial_weights`, and its starting weight values will be\n",
    "        taken directly from that same parameter. If `self.use_batch_norm` is True, this\n",
    "        layer will include batch normalization, otherwise it will not. \n",
    "        \n",
    "        :param layer_in: Tensor\n",
    "            The Tensor that feeds into this layer. It's either the input to the network or the output\n",
    "            of a previous layer.\n",
    "        :param initial_weights: NumPy array or Tensor\n",
    "            Initial values for this layer's weights. The shape defines the number of nodes in the layer.\n",
    "            e.g. Passing in 3 matrix of shape (784, 256) would create a layer with 784 inputs and 256 \n",
    "            outputs. \n",
    "        :param activation_fn: Callable or None (default None)\n",
    "            The non-linearity used for the output of the layer. If None, this layer will not include \n",
    "            batch normalization, regardless of the value of `self.use_batch_norm`. \n",
    "            e.g. Pass tf.nn.relu to use ReLU activations on your hidden layers.\n",
    "        \"\"\"\n",
    "        # Since this class supports both options, only use batch normalization when\n",
    "        # requested. However, do not use it on the final layer, which we identify\n",
    "        # by its lack of an activation function.\n",
    "        if self.use_batch_norm and activation_fn:\n",
    "            # Batch normalization uses weights as usual, but does NOT add a bias term. This is because \n",
    "            # its calculations include gamma and beta variables that make the bias term unnecessary.\n",
    "            # (See later in the notebook for more details.)\n",
    "            weights = tf.Variable(initial_weights)\n",
    "            linear_output = tf.matmul(layer_in, weights)\n",
    "\n",
    "            # Apply batch normalization to the linear combination of the inputs and weights\n",
    "            batch_normalized_output = tf.layers.batch_normalization(linear_output, training=self.is_training)\n",
    "\n",
    "            # Now apply the activation function, *after* the normalization.\n",
    "            return activation_fn(batch_normalized_output)\n",
    "        else:\n",
    "            # When not using batch normalization, create a standard layer that multiplies\n",
    "            # the inputs and weights, adds a bias, and optionally passes the result \n",
    "            # through an activation function.  \n",
    "            weights = tf.Variable(initial_weights)\n",
    "            biases = tf.Variable(tf.zeros([initial_weights.shape[-1]]))\n",
    "            linear_output = tf.add(tf.matmul(layer_in, weights), biases)\n",
    "            return linear_output if not activation_fn else activation_fn(linear_output)\n",
    "\n",
    "    def train(self, session, learning_rate, training_batches, batches_per_sample, save_model_as=None):\n",
    "        \"\"\"\n",
    "        Trains the model on the MNIST training dataset.\n",
    "        \n",
    "        :param session: Session\n",
    "            Used to run training graph operations.\n",
    "        :param learning_rate: float\n",
    "            Learning rate used during gradient descent.\n",
    "        :param training_batches: int\n",
    "            Number of batches to train.\n",
    "        :param batches_per_sample: int\n",
    "            How many batches to train before sampling the validation accuracy.\n",
    "        :param save_model_as: string or None (default None)\n",
    "            Name to use if you want to save the trained model.\n",
    "        \"\"\"\n",
    "        # This placeholder will store the target labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.output_layer))\n",
    "        \n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        if self.use_batch_norm:\n",
    "            # If we don't include the update ops as dependencies on the train step, the \n",
    "            # tf.layers.batch_normalization layers won't update their population statistics,\n",
    "            # which will cause the model to fail at inference time\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        else:\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "        # Train for the appropriate number of batches. (tqdm is only for a nice timing display)\n",
    "        for i in tqdm.tqdm(range(training_batches)):\n",
    "            # We use batches of 60 just because the original paper did. You can use any size batch you like.\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(60)\n",
    "            session.run(train_step, feed_dict={self.input_layer: batch_xs, \n",
    "                                               labels: batch_ys, \n",
    "                                               self.is_training: True})\n",
    "        \n",
    "            # Periodically test accuracy against the 5k validation images and store it for plotting later.\n",
    "            if i % batches_per_sample == 0:\n",
    "                test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                                 labels: mnist.validation.labels,\n",
    "                                                                 self.is_training: False})\n",
    "                self.training_accuracies.append(test_accuracy)\n",
    "\n",
    "        # After training, report accuracy against test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.validation.images,\n",
    "                                                         labels: mnist.validation.labels,\n",
    "                                                         self.is_training: False})\n",
    "        print('{}: After training, final accuracy on validation set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If you want to use this model later for inference instead of having to retrain it,\n",
    "        # just construct it with the same parameters and then pass this file to the 'test' function\n",
    "        if save_model_as:\n",
    "            tf.train.Saver().save(session, save_model_as)\n",
    "\n",
    "    def test(self, session, test_training_accuracy=False, include_individual_predictions=False, restore_from=None):\n",
    "        \"\"\"\n",
    "        Trains a trained model on the MNIST testing dataset.\n",
    "\n",
    "        :param session: Session\n",
    "            Used to run the testing graph operations.\n",
    "        :param test_training_accuracy: bool (default False)\n",
    "            If True, perform inference with batch normalization using batch mean and variance;\n",
    "            if False, perform inference with batch normalization using estimated population mean and variance.\n",
    "            Note: in real life, *always* perform inference using the population mean and variance.\n",
    "                  This parameter exists just to support demonstrating what happens if you don't.\n",
    "        :param include_individual_predictions: bool (default True)\n",
    "            This function always performs an accuracy test against the entire test set. But if this parameter\n",
    "            is True, it performs an extra test, doing 200 predictions one at a time, and displays the results\n",
    "            and accuracy.\n",
    "        :param restore_from: string or None (default None)\n",
    "            Name of a saved model if you want to test with previously saved weights.\n",
    "        \"\"\"\n",
    "        # This placeholder will store the true labels for each mini batch\n",
    "        labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "        # Define operations for testing\n",
    "        correct_prediction = tf.equal(tf.argmax(self.output_layer, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        # If provided, restore from a previously saved model\n",
    "        if restore_from:\n",
    "            tf.train.Saver().restore(session, restore_from)\n",
    "\n",
    "        # Test against all of the MNIST test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict={self.input_layer: mnist.test.images,\n",
    "                                                         labels: mnist.test.labels,\n",
    "                                                         self.is_training: test_training_accuracy})\n",
    "        print('-'*75)\n",
    "        print('{}: Accuracy on full test set = {}'.format(self.name, test_accuracy))\n",
    "\n",
    "        # If requested, perform tests predicting individual values rather than batches\n",
    "        if include_individual_predictions:\n",
    "            predictions = []\n",
    "            correct = 0\n",
    "\n",
    "            # Do 200 predictions, 1 at a time\n",
    "            for i in range(200):\n",
    "                # This is a normal prediction using an individual test case. However, notice\n",
    "                # we pass `test_training_accuracy` to `feed_dict` as the value for `self.is_training`.\n",
    "                # Remember that will tell it whether it should use the batch mean & variance or\n",
    "                # the population estimates that were calucated while training the model.\n",
    "                pred, corr = session.run([tf.arg_max(self.output_layer,1), accuracy],\n",
    "                                         feed_dict={self.input_layer: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    self.is_training: test_training_accuracy})\n",
    "                correct += corr\n",
    "\n",
    "                predictions.append(pred[0])\n",
    "\n",
    "            print(\"200 Predictions:\", predictions)\n",
    "            print(\"Accuracy on 200 samples:\", correct/200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
